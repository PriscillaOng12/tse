# Implementation Specification and Testing Plan for the Querier Module

## Implementation Details
The Querier module is designed to efficiently process user queries against an index generated by the Indexer, matching those queries against documents crawled by the Crawler. The implementation leverages several key data structures and algorithms:

- **Index Loading**: The index is loaded from a file using a hashtable data structure, where each entry corresponds to a word and its associated counters data structure, which maps document IDs to the frequency of the word in that document.
- **Query Parsing**: User queries are parsed and tokenized, taking into account logical operators "and" and "or". The parsing process also involves normalization of query terms to ensure consistency with the indexed data.
- **Query Processing**: Queries are processed by sequentially applying "and" and "or" operations as per the precedence rules. "And" operations are evaluated first, combining document frequencies using intersection, followed by "or" operations, which union the results of "and" blocks.
- **Result Ranking and Output**: Documents matching the query are ranked based on the number of occurrences of query terms, and results are printed to the user, showing document IDs and the paths to the documents.

## Testing Plan
The testing plan for the Querier involves both functional and memory leak testing, as outlined in the provided testing script `testing.sh` on letters at depts 1, 2, and 3, toscrape at depths 1, 2, and 3, and wikipedia at depths 1, 2, and 3.

**testing.sh assumed existence of crawler directories from cs50dev shared output files: `../../shared/tse/output/letters-1`, `../../shared/tse/output/letters-2`, `../../shared/tse/output/letters-3`, `../../shared/tse/output toscrape-1`, `../../shared/tse/output/toscrape-2`, `../../shared/tse/output/toscrape-3`, `../../shared/tse/output/wikipedia-1`, `../../shared/tse/output/wikipedia-2`, and `../../shared/tse/output/wikipedia-3`**

Functional testing portion involves running `BASIC_TEST` and `fuzzquery`. 

`BASIC_TEST` involves testings for words normalization, basic conjuctions, and precendence test with simple queries.

`fuzzquery` generate a series of random queries for testing querier.

`valgrind` is run on querier for letters at depth 3, toscrape at depth 3, and wikipedia at depth 3, to test for memory leaks.

### Functional Testing
Functional tests are designed to cover various scenarios, including:

- **Invalid Arguments**: Tests are run without arguments, with insufficient arguments, and with excessive arguments to ensure that the Querier handles argument errors gracefully.
- **Invalid Directories and Files**: The Querier is tested with non-existent directories, directories that are not crawler-produced, and non-existent index files to verify error handling.
- **Valid Queries**: The Querier is tested against known good data sets with predefined queries to ensure accurate query processing and result output. This includes basic queries without logical operators, as well as more complex queries involving "and" and "or" operators.
- **Fuzzy Queries**: The `fuzzquery` utility is used to generate random queries, testing the Querier's robustness and its ability to handle unexpected input.

### Memory Leak Testing
Valgrind is used to check for memory leaks and errors:

- **Leak Checks**: After functional tests, Valgrind is run with `--leak-check=full` and `--show-leak-kinds=all` options to detect any memory leaks or errors in the Querier, ensuring memory safety and correct memory management.

### Test Output Files
For each test case, output is redirected to a corresponding `.out` file (e.g., `letters-1-basic-test.out`, `toscrape-1-fuzzquery-test.out`), which can be reviewed manually to verify correct behavior and to compare against expected results.

### Test Cleanup
The testing script ensures cleanup of any temporary directories or files created during testing, maintaining a clean testing environment.

## Conclusion
This implementation and testing plan for the Querier module aim to ensure comprehensive coverage of functionality and memory safety, providing confidence in the Querier's reliability and correctness in processing search queries against a web page index.
